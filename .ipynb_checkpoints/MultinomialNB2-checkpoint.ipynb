{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97caa945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "nltk.download('wordnet')\n",
    "\n",
    "my_path = os.getcwd()\n",
    "os.chdir(my_path)\n",
    "file = 'training-Obama-Romney-tweets.xlsx'\n",
    "\n",
    "\n",
    "obama = 'Obama'\n",
    "romney = 'Romney'\n",
    "\n",
    "tweets_df = pd.read_excel(file, sheet_name = obama)\n",
    "# dfs = {sheet_name: tweets.parse(sheet_name) \n",
    "#           for sheet_name in tweets.sheet_names}\n",
    "\n",
    "tweets_df.dropna(subset = ['Unnamed: 4'], inplace=True)\n",
    "tweets_df['Unnamed: 4']=tweets_df[\"Unnamed: 4\"].map(str)\n",
    "\n",
    "\n",
    "tweets_df = tweets_df.drop(tweets_df[tweets_df['Unnamed: 4']=='!!!!'].index,axis=0)\n",
    "tweets_df = tweets_df.drop(tweets_df[tweets_df['Unnamed: 4']=='irrevelant'].index,axis=0)\n",
    "tweets_df = tweets_df.drop(tweets_df[tweets_df['Unnamed: 4']=='irrelevant'].index,axis=0)\n",
    "tweets_df = tweets_df.drop(tweets_df[tweets_df['Unnamed: 4']=='2'].index,axis=0)\n",
    "tweets_df = tweets_df.drop(0,axis=0)\n",
    "tweets_df['Unnamed: 4']=tweets_df[\"Unnamed: 4\"].map(int)\n",
    "tweets_df['Anootated tweet'] = tweets_df['Anootated tweet'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "\n",
    "X=tweets_df['Anootated tweet']\n",
    "Y=tweets_df['Unnamed: 4']\n",
    "\n",
    "l1 = []\n",
    "for i in range(len(Y)):\n",
    "    if Y.iloc[i] not in l1:\n",
    "        l1.append(Y.iloc[i])\n",
    "print(\"y is :\",l1)\n",
    "\n",
    "tweets_df['Unnamed: 4'].unique()\n",
    "\n",
    "# # Plotting the distribution for dataset.\n",
    "# ax = tweets_df.groupby('Unnamed: 4').count().plot(kind='bar', title='Distribution of data',legend=False)\n",
    "# ax.set_xticklabels(['Neutral','Positive','Negative'], rotation=0)\n",
    "# # Storing data in lists.\n",
    "# text, sentiment = list(tweets_df['Anootated tweet']), list(tweets_df['Unnamed: 4'])\n",
    "\n",
    "# sns.countplot(x='Unnamed: 4', data=tweets_df)\n",
    "\n",
    "\n",
    "\n",
    "data=tweets_df[['Anootated tweet','Unnamed: 4']]\n",
    "data['Unnamed: 4'].unique()\n",
    "\n",
    "data_pos = data[data['Unnamed: 4'] == 1]\n",
    "data_neg = data[data['Unnamed: 4'] == -1]\n",
    "data_neu = data[data['Unnamed: 4'] == 0]\n",
    "\n",
    "dataset = pd.concat([data_pos, data_neg, data_neu])\n",
    "\n",
    "dataset['Anootated tweet']=dataset['Anootated tweet'].str.lower()\n",
    "# dataset['Anootated tweet'].tail()\n",
    "\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "STOPWORDS = list(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(lambda text: cleaning_stopwords(text))\n",
    "dataset['Anootated tweet'].head()\n",
    "\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "dataset['Anootated tweet']= dataset['Anootated tweet'].apply(lambda x: cleaning_punctuations(x))\n",
    "dataset['Anootated tweet'].tail()\n",
    "\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(lambda x: cleaning_repeating_char(x))\n",
    "dataset['Anootated tweet'].tail()\n",
    "\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(lambda x: cleaning_URLs(x))\n",
    "dataset['Anootated tweet'].tail()\n",
    "\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['Anootated tweet'].tail()\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(tokenizer.tokenize)\n",
    "\n",
    "\n",
    "st = PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return text\n",
    "\n",
    "dataset['Anootated tweet']= dataset['Anootated tweet'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['Anootated tweet']\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return text\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['Anootated tweet']\n",
    "\n",
    "X = dataset['Anootated tweet']\n",
    "\n",
    "cleaned_data = []\n",
    "for i in range(len(X)):\n",
    "    tweet = re.sub('[^a-zA-Z]',' ',str(X.iloc[i]))\n",
    "    tweet = tweet.split()\n",
    "    tweet = ' '.join(tweet)\n",
    "    cleaned_data.append(tweet)\n",
    "\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2),stop_words=['obama','barack','romney','mitt'])\n",
    "\n",
    "\n",
    "X_fin=vectoriser.fit_transform(cleaned_data).toarray()\n",
    "y = dataset['Unnamed: 4']\n",
    "\n",
    "# #MultinomialNB\n",
    "# X_train,X_test,y_train,y_test=train_test_split(X_fin,y,test_size=0.3,random_state=26105111)\n",
    "# model=MultinomialNB()\n",
    "# model.fit(X_train,y_train)\n",
    "# y_pred=model.predict(X_test)\n",
    "# cf=classification_report(y_test,y_pred)\n",
    "# print(cf)\n",
    "\n",
    "\n",
    "# #SVM Model\n",
    "# X_train,X_test,y_train,y_test=train_test_split(X_fin,y,test_size=0.3,random_state=26105111)\n",
    "# SVCmodel = LinearSVC()\n",
    "# SVCmodel.fit(X_train, y_train)\n",
    "# model_Evaluate(SVCmodel)\n",
    "# y_pred2 = SVCmodel.predict(X_test)\n",
    "# cf=classification_report(y_test,y_pred)\n",
    "# print(cf)\n",
    "\n",
    "#LogisticRegression Model\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_fin,y,test_size=0.3,random_state=26105111)\n",
    "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "y_pred3 = LRmodel.predict(X_test)\n",
    "cf=classification_report(y_test,y_pred)\n",
    "print(cf)\n",
    "\n",
    "# BNBmodel = BernoulliNB()\n",
    "# BNBmodel.fit(X_train, y_train)\n",
    "# y_pred1 = BNBmodel.predict(X_test)\n",
    "\n",
    "# cf=classification_report(y_test,y_pred)\n",
    "# print(cf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d764b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4d914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18efbb19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
