{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "97caa945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aikyab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is : ['0', '1', '-1']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.60      0.59       590\n",
      "           1       0.53      0.45      0.48       603\n",
      "           2       0.54      0.61      0.57       495\n",
      "\n",
      "    accuracy                           0.55      1688\n",
      "   macro avg       0.55      0.55      0.55      1688\n",
      "weighted avg       0.55      0.55      0.55      1688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import os\n",
    "\n",
    "my_path = os.getcwd()\n",
    "os.chdir(my_path)\n",
    "file = 'training-Obama-Romney-tweets.xlsx'\n",
    "\n",
    "\n",
    "obama = 'Obama'\n",
    "romney = 'Romney'\n",
    "\n",
    "tweets_df = pd.read_excel(file, sheet_name = obama)\n",
    "# dfs = {sheet_name: tweets.parse(sheet_name) \n",
    "#           for sheet_name in tweets.sheet_names}\n",
    "\n",
    "tweets_df.dropna(subset = ['Unnamed: 4'], inplace=True)\n",
    "tweets_df['Unnamed: 4']=tweets_df[\"Unnamed: 4\"].map(str)\n",
    "\n",
    "\n",
    "tweets_df = tweets_df.drop(tweets_df[tweets_df['Unnamed: 4']=='!!!!'].index,axis=0)\n",
    "tweets_df = tweets_df.drop(tweets_df[tweets_df['Unnamed: 4']=='irrevelant'].index,axis=0)\n",
    "tweets_df = tweets_df.drop(tweets_df[tweets_df['Unnamed: 4']=='irrelevant'].index,axis=0)\n",
    "tweets_df = tweets_df.drop(tweets_df[tweets_df['Unnamed: 4']=='2'].index,axis=0)\n",
    "tweets_df = tweets_df.drop(0,axis=0)\n",
    "\n",
    "tweets_df['Anootated tweet'] = tweets_df['Anootated tweet'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "\n",
    "\n",
    "\n",
    "X=tweets_df['Anootated tweet']\n",
    "Y=tweets_df['Unnamed: 4']\n",
    "\n",
    "l1 = []\n",
    "for i in range(len(Y)):\n",
    "    if Y.iloc[i] not in l1:\n",
    "        l1.append(Y.iloc[i])\n",
    "print(\"y is :\",l1)\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "l1 = []\n",
    "for i in range(len(Y)):\n",
    "    if Y.iloc[i] not in l1:\n",
    "        l1.append(Y.iloc[i])\n",
    "# print(\"y is :\",l1)\n",
    "\n",
    "cleaned_data=[]\n",
    "\n",
    "for i in range(len(X)):\n",
    "    tweet = re.sub('[^a-zA-Z]',' ',str(X.iloc[i]))\n",
    "    tweet = tweet.lower().split()\n",
    "    tweet = [stemmer.stem(word) for word in tweet if (word not in stop_words)]\n",
    "    tweet = ' '.join(tweet)\n",
    "    cleaned_data.append(tweet)\n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=3000,stop_words=['obama','barack'])\n",
    "X_fin=cv.fit_transform(cleaned_data).toarray()\n",
    "\n",
    "sentiment_ordering = ['-1', '0', '1']\n",
    "y = Y.apply(lambda x: sentiment_ordering.index(x))\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model=MultinomialNB()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_fin,y,test_size=0.3)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "cf=classification_report(y_test,y_pred)\n",
    "\n",
    "print(cf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "10d764b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is : ['0', '1', '-1']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.47      0.41       567\n",
      "           1       0.41      0.38      0.39       618\n",
      "           2       0.32      0.24      0.28       503\n",
      "\n",
      "    accuracy                           0.37      1688\n",
      "   macro avg       0.36      0.36      0.36      1688\n",
      "weighted avg       0.37      0.37      0.36      1688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cdabcb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/aikyab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-4016e3a63c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0my_pred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBNBmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0mcf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m         \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "tweets_df['Unnamed: 4'].unique()\n",
    "\n",
    "# # Plotting the distribution for dataset.\n",
    "# ax = tweets_df.groupby('Unnamed: 4').count().plot(kind='bar', title='Distribution of data',legend=False)\n",
    "# ax.set_xticklabels(['Neutral','Positive','Negative'], rotation=0)\n",
    "# # Storing data in lists.\n",
    "# text, sentiment = list(tweets_df['Anootated tweet']), list(tweets_df['Unnamed: 4'])\n",
    "\n",
    "# sns.countplot(x='Unnamed: 4', data=tweets_df)\n",
    "\n",
    "\n",
    "\n",
    "data=tweets_df[['Anootated tweet','Unnamed: 4']]\n",
    "data['Unnamed: 4'].unique()\n",
    "\n",
    "data_pos = data[data['Unnamed: 4'] == '1']\n",
    "data_neg = data[data['Unnamed: 4'] == '-1']\n",
    "data_neu = data[data['Unnamed: 4'] == '0']\n",
    "\n",
    "dataset = pd.concat([data_pos, data_neg, data_neu])\n",
    "\n",
    "dataset['Anootated tweet']=dataset['Anootated tweet'].str.lower()\n",
    "# dataset['Anootated tweet'].tail()\n",
    "\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "STOPWORDS = list(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(lambda text: cleaning_stopwords(text))\n",
    "dataset['Anootated tweet'].head()\n",
    "\n",
    "\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "dataset['Anootated tweet']= dataset['Anootated tweet'].apply(lambda x: cleaning_punctuations(x))\n",
    "dataset['Anootated tweet'].tail()\n",
    "\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(lambda x: cleaning_repeating_char(x))\n",
    "dataset['Anootated tweet'].tail()\n",
    "\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(lambda x: cleaning_URLs(x))\n",
    "dataset['Anootated tweet'].tail()\n",
    "\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['Anootated tweet'].tail()\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(tokenizer.tokenize)\n",
    "\n",
    "\n",
    "st = PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return text\n",
    "\n",
    "dataset['Anootated tweet']= dataset['Anootated tweet'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['Anootated tweet']\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return text\n",
    "dataset['Anootated tweet'] = dataset['Anootated tweet'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['Anootated tweet']\n",
    "\n",
    "X = dataset['Anootated tweet']\n",
    "\n",
    "cleaned_data = []\n",
    "for i in range(len(X)):\n",
    "    tweet = re.sub('[^a-zA-Z]',' ',str(X.iloc[i]))\n",
    "    tweet = tweet.lower().split()\n",
    "    tweet = [stemmer.stem(word) for word in tweet if (word not in stop_words)]\n",
    "    tweet = ' '.join(tweet)\n",
    "    cleaned_data.append(tweet)\n",
    "    \n",
    "# X = data['Anootated tweet']\n",
    "\n",
    "\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2),stop_words=['obama','barack','romney','mitt'])\n",
    "\n",
    "X_fin=vectoriser.fit_transform(cleaned_data).toarray()\n",
    "y = dataset['Unnamed: 4']\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_fin,y,test_size=0.3,random_state=26105111)\n",
    "\n",
    "X_train.shape\n",
    "y_train.shape\n",
    "\n",
    "\n",
    "BNBmodel = BernoulliNB()\n",
    "# X_train.shape\n",
    "# y_train.shape\n",
    "BNBmodel.fit(X_train, y_train)\n",
    "model_Evaluate(BNBmodel)\n",
    "y_pred1 = BNBmodel.predict(X_test)\n",
    "\n",
    "cf=classification_report(y_test,y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c973333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
